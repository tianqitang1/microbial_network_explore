---
title: "Network inference for microbial community"
bibliography: bib.bib
link-citations: yes
editor: 
    render-on-save: true
csl: csl.csl
---
#
<!-- March 3rd, Introduction part -->
In this study, we are trying to assess the performance of different network inference methods for microbial community time-series data.

The first difficulty is the compositinality of the data, which is a common problem in microbiome analysis.
When sequencing the microbiome, the reads are first amplified, and then sequenced.
The sequenced reads are grouped into operational taxonomic units (OTUs) or amplicon sequence variants (ASVs) based on the similarity of the sequences using computational methods.
All steps introduce compositonality into the data, making the final abundance numbers relative to the total number of reads, which is not the absolute abundance.

The second difficulty is the inconsistency between the high dimensions of the data and the low number of time points.
In the SPOT dataset, there number of ASVs is over 70,000 but the sampling is only conducted monthly, it is impractical to hope for a daily, or even hourly sampling.
If we throw away the taxa showing significant statistical characteristics, or taxa with high sparsity (0 for most of the time points), the dimension of the data will be reduced.
However, this risk losing potential important taxa, there was one  [liv's paper], there might also be dominant taxon which makes its abundance variance small, but contribute a lot to the community dynamics.

The experiment of assessing different network inference methods can be split into two folds: species abundance time-series simulation and solving and evaluating the network.

## Simulation methods {#sec-simulation}

The generalized Lotka-Volterra (gLV) model is the most widely used method for simulating a microbial community.
MiSDEED [@Chlenski_Hsu_Peer_2022] is a Python package based on gLV, in addition to regular gLV simulations, it also considers the environmental perturbations/interventions.
After the absolute abundance of species $\textbf{Z}$ is generated, MiSDEED also calculates the relative abundance $\textbf{Y}$, and finally samples the observed relative abundance $\textbf{X}$ to simulate the read sampling.

In the gLV simulation, there are multiple adjustable parameters, including the growth rate $g$, the interaction matrix $\textbf{M}$ and the perturbation over time.
Among these parameters, the adjacency matrix $\textbf{A}$ determines the structure of the network, and the weight matrix $\textbf{M}$ determines the interaction between two species where $\textbf{M}_{ij}$ indicates the effect of species $i$ to species $j$.

As the parameter of the generated network, suppose there are $n$ species in the network, and the average degree of the network is $\langle k \rangle = \frac{2m}{n}$, where $m$ is the total number of edges in the network.

Different models were proposed to generate the adjacency matrix $\textbf{A}$, the most straightforward way is to generate a random network, where two nodes are connected with a probability $p$.
However, the random network is not a good representation of the real microbial community; the real network usually has two properties: small-world and scale-free.

- Random network

    This type of network is homogeneous in most properties and has no special structure.
    However, networks in real life exhibit a variety of structures, and the random network is not a good representation of the real microbial community.

- Small-world network

    When taxon $i$ is connected to both taxon $j$ and taxon $k$, there is a higher probability that taxon $j$ and taxon $k$ are connected too.
    This type of network can be generated with the Watts-Strogatz model [@Watts_Strogatz_1998].

- Scale-free network

    In real complex network it is very likely that a small number of nodes, called "hubs", have a large number of connections, while most nodes have only a few connections.
    The distribution of the number of connections of the nodes in the network follows a power law.
    This type of network can be generated with the Chung-Lu model [@Chung_Lu_2002].

    <!-- add scale-free challenge -->

## Network inference methods {#sec-methods}

The network inference methods currently used are listed below.

<!-- [to fill different methods]{style="color: red"} -->
- Pearson

- Pearson partial

- SparCC

- SPIEC-EASI

    Sparse Inverse Covariance Estimation for Ecological Association Studies (SPIEC-EASI) [@Kurtz_M端ller_Miraldi_Littman_Blaser_Bonneau_2015] aims to infer the interaction network based on probabilistic graphical models.
    Traditional correlation based methods are likely to induce false positive correlations for non-directly connected nodes, SPIEC-EASI uses the inverse covariance matrix (also called precision matrix), which were shown to remove the indirect connections [@Koller_Friedman_2009], to infer the network structure.
    Regarding the compositionality of the data, SPIEC-EASI preprocess the data with centered log-ratio transformation (CLR).

    There are more to the methods implemented in SPIEC-EASI, for inferring the graphical model, two methods were implemented: the neighbourhood selection framework using Meinshausen and B端hlmann (MB) algorithm [@Meinshausen_B端hlmann_2006], is essentially treat the problem as a regularized linear regression problem (LASSO).
    The second method focuses on the estimation of the inverse covariance matrix, and it is shown in the paper that this problem can be reduced to a convex optimization problem.

<!-- Feb 28 2023, added clv -->
- Compositional Lotka-Volterra (cLV)

    The compositional Lotka-Volterra (cLV) model [@Joseph_Shenhav_Xavier_Halperin_Peer_2020] is a model that can be used to infer the network structure from the relative abundance data.
    The code to reproduce the result in [@Joseph_Shenhav_Xavier_Halperin_Peer_2020] is available at [GitHub](https://github.com/tyjo/clv), but it is not well documented and the code is not well organized.
    After digging through the codes, we can see that the main module for compting the cLV model is located in `compositional_lotka_volterra.py`, the authors also implemented the generalized Lotka-Volterra (gLV) model in `generalized_lotka_volterra.py`.

    <!-- In the paper, the author solves the following optimization problem to infer the network structure. -->

    The program predicts the interaction matrix $\textbf{A'}'$, under assumption of either the gLV or cLV model, and then we can calculate the corresponding PR-AUC using the ground truth adjacency matrix $\textbf{A}$.

    




<!-- March 1 2023, section about evaluation -->
## Performance evaluation {#sec-evaluation}

It is difficult to evaluate the performance of different network inference methods, as different methods are developed with different purposes and it is hard to find a common ground for comparison.
Different evaluation methods should also be chosen according to the purpose of the network inference and different network assumptions.

First of all, the network topology is the most important property of the network, determined by the adjacency matrix $\textbf{A}$.
If the network topology is the main focus, we can use the area under precision-recall curve, PR-AUC, to evaluate the performance of different methods.
Given the result of each method, we first take the absolute value of the result, and the calculate the precision-recall curve accordingly.

It is easy to see by taking the absolute value we removed all information about the type of interaction between taxa.
Because we are generating the simulated data under Lotka-Volterra model, the compositional Lotka-Volterra (cLV) model implemented in [@Joseph_Shenhav_Xavier_Halperin_Peer_2020] is the only method that we can use the root mean square error (RMSE) to evaluate the performance, because they are under the exact same assumptions.
For methods that can do prediction, RMSE is also a good choice to evaluate the performance by comparing the predicted values with the true values, but in our current stage we are not focusing on the prediction part.

Global network properties, such as the average degree, the clustering coefficient, the average and shortest path length, and the degree distribution, are also important properties of the network.
The following paragraphs will describe a few used in [@Kurtz_M端ller_Miraldi_Littman_Blaser_Bonneau_2015].
For comparison among different methods, the authors selected the top 205 predicted edges and compared it to the true network with 205 OTUs (why 205?).

- Node degree distributions

    Different network types have different degree distributions.
    The random network has a uniform degree distribution, while the scale-free network has a power-law degree distribution.
    To make a comparison between the distribution of predicted degrees and the true degrees, the authors used the Kullback-Leibler divergence (KL divergence) to measure the difference between the two distributions.

- Betweenness centrality


## Preliminary results

### Comparison between naive gLV model and MiSDEED

In the analysis of [@Hirano_Takemoto_2019], the authors used the naive gLV model to generate the network, whilst the MiSDEED package was able to generate more realistic simulation by considering reads sampling, random noises and environmental perturbations.
One natural question arises is that whether the evaluation results in [@Hirano_Takemoto_2019] holds for networks generated by MiSDEED.

As a preliminary test, we set the number of taxa in the network to be $n = 20$, and the average degree of the network to be $\langle k \rangle = 2$.
The network structure was set to random network.
The tested results are Pearson's correlation, Pearson's partial correlation, SparCC, and SPIEC-EASI.
The performance of different methods are evaluated using PR-AUC, which is the area under the precision-recall curve, and the result is shown in the following table.


|                 | naive     | MiSDEED    |
|-----------------|-----------|------------|
| Pearson         | 0.6833395 | 0.07341247 |
| Pearson partial | 0.6766284 | 0.08995016 |
| SparCC          | 0.6300331 | 0.1027987  |
| SPIEC-EASI      | 0.7314267 | 0.09747966 |

The result shows that the performance of different methods on data generated by the naive gLV model is much better than data generated by MiSDEED, which invalidates nearly all the practical uses of the current available methods.

### Exploration of the effect of different factors to the performance of different methods

The major difference between the naive data and the data generated by MiSDEED is the noise introduced to the data.
MiSDEED generates three matrices in the process of the simulation: the absolute abundance matrix $\textbf{Z}$, the relative abundance matrix $\textbf{Y}$ and the observed relative abundance matrix $\textbf{X}$.
The absolute abundance matrix $\textbf{Z}$ is generated by the gLV model, and the relative abundance matrix $\textbf{Y}$ is calculated by dividing the absolute abundance matrix $\textbf{Z}$ by the sum of each row, and the observed relative abundance matrix $\textbf{X}$ is calculated by sampling the relative abundance matrix $\textbf{Y}$ with a Multinomial distribution.

For the naive data, on the other hand, can be considered only the absolute abundance matrix $\textbf{Z}$, with only simple downsampling, it is natrual for us wonder how the noise introduced by MiSDEED affects the performance of different methods.

## March 8th 2023 report

For the current phase of experiment, I am only evaluating the PR-AUC of different methods, other metrics mentioned in the performance evaluation section above are not considered for now.

In my repeated experiment of testing different methods, I observed that the performance of different methods are highly dependent on the parameters selected.
The following table can just be a special case, more experiments are needed to summarize the general trend.
For this particular experiment, I set the number of taxa in the network to be $n = 50$, and the average degree of the network to be $\langle k \rangle = 5$, in total $t=100$ time points were sampled using both the naive gLV model and MiSDEED, with greater noise introduced into the data.

First of all, before comparing different methods, we need to draw a baseline.
To generate the baseline, a $50 \times 50$ matrix with each entry being a random number between 0 and 1 is generated, and then the corresponding PR-AUC is calculated.
In total 1000 such matrices are generated, and the average PR-AUC is collected, the average PR-AUC score is used as the baseline.
For the current parameters setting, the baseline is 0.105.
However, the baseline is highly dependent on the parameters selected, for example, if we suse $n = 25, \langle\rangle = 10, t=10$, the baseline can be as high as 0.42.
Therefore, it is important for us to compare to the baseline when we are presenting our results.

As for the actual result I got on the current parameters setting, the following table shows the PR-AUC score of different methods.

|                  | naive | MiSDEED |
|------------------|-------|---------|
| Pearson          | 0.539 | 0.110   |
| Precision Matrix | 0.203 | 0.109   |
| Pearson partial  | 0.396 | 0.131   |
| SPARCC           | 0.236 | 0.180   |
| SPIEC-EASI (MB)  | 0.425 | 0.438   |
| cLV              | 0.118 | 0.109   |
| gLV              | 0.125 | 0.113   |

From the table, we can see that the time-series dependent methods, cLV and gLV, are the worst performers.
Because the code distributed by the authors are poorly documented, and from what I see in the code, there are quite a few inssues in their implementation, so we do not know if the poor performance is due to the implementation or the method itself.
The performance of the other methods are not that bad on the naive data, although it is suprising to see that the Pearson method is performs the best.
For the MiSDEED data, the performance tanked for all the methods, but the SPIEC-EASI method performed even better than the naive data, comparing to the result of previous experiment which is also in this document, this result is quite suprising, this can be another indicator of the instability of the experiment, and further experiments are needed to confirm it.


## March 22nd 2023 report

In the previous experiment, I only tested the performance of different methods on the naive data and the MiSDEED data with limited combination of parameters, and only reported the PR-AUC score.
In this report, I included different combination of parameters, and also included ROC-AUC score, due to the large amount of parameter combination, I will only discuss a few of the results.

For the parameters, we have to choose: the number of taxa $n$, the average degree $\langle k \rangle$, the variance of the noise $\sigma$.
For the number of taxa, I tested $n = \{20, 50, 100\}$.
For the average degree, I tested $\langle k \rangle = \{2, 5, 10\}$.
For the variance of the noise, I tested $\sigma = \{1e-5, 1e-4, 1e-3, 1e-2, 1e-1\}$.
For simulation method, I tested both the naive gLV model and MiSDEED, in addition, I also tested the naive gLV model with normal noise added to the data, as nearly none of the methods can handle the data generated by MiSDEED, I wanted to verify if it is because the data by MiSDEED is too hard, or because the methods are not robust to any noise.

The network tyepe was fixed to be random network, as there was reference showed that small-world and scale-free network would be even more difficult to infer, according to the results to be shown later, there isn't many points to test on the other network type if the current methods cannot even handle the random network.

Firstly, to show why we want to use the ROC-AUC score instead of the PR-AUC score, I first plot the baseline ROC-AUC and PR-AUC score calculated on 1000 randomly generated data with different number of taxa.

![ROC-AUC and PR-AUC score of baseline](plots/baseline.png)

The left panel is PR-AUC score, and the right panel is ROC-AUC score.
We can easily see that the PR-AUC score is significantly affected by the number of taxa, while the ROC-AUC score is not.
I only showed the effect of the number of taxa, but the same trend can be observed when we change the average degree.
Therefore, using ROC-AUC score will be easier for us to compare the effectiveness of different methods.

Next, I plot the ROC-AUC score of different methods on the naive data and naive data with normal noise added.
The x-axis is the change of noise variance, and the y-axis is the ROC-AUC score, the color and order of the bars are the method used, different results of all other parameters (number of taxa, average degree) are depicted in this figure as the error bar.

![ROC-AUC score on naive data](plots/noise_var.png)

Surprisingly for the naive data, the performance of all the methods are quite similar, and the performance is not significantly affected by the noise variance. 

Then for the simulation generated by MiSDEED, the bar chart for ROC-AUC score with different noise variance is as follows, the result of naive data with no noise is also included as reference.

![ROC-AUC score on MiSDEED data](plots/misdeed.png)

Basically for MiSDEED data, all methods are just not working, this could be attributed to the more sophisticated yet more realistic simulation of MiSDEED

<!-- - feasibility of the problem

- bistability using extended LV model

- do we need to implement our own simulation method? -->

<!-- ## Idea

Can the problem of high degree node be solved by using Granger causality? -->


## March 29th 2023 report

As we discussed last week, we are currently using the $z$ matrix generated by MiSDEED, which is the absolute read count, to evaluate the different methods.
In this week's report, I mainly focused on three different variables: total timepoints, maximal interaction strength, and time step between two timepoints ($dt$).
The downsample parameter in MiSDEED is deprecated by the author, because its effect is equivalent to changing $dt$.

First we explore the effect of the number of timepoints on the AUROC score.
The possible number of timepoints are 100, 1000, 5000 and 10000.

![AUROC score on different number of timepoints](plots/roc_auc_score_timepoints.png)

We can observe that for gLV, the AUROC score is increasing with the number of timepoints.
For other methods, the score is not showing any clear trend, as they are not quite working on our simulated dataset.

The second experiment I conducted is to explore the effect of the maximal interaction strength ($m$) on the AUROC score.
When simulating the dataset, the interaction strength is randomly generated from a uniform distribution between $-m$ and $m$.
The possible values of $m$ are 0.1, 0.5 and 1, values greater than 1 will cause error.

![AUROC score on different maximal interaction strength](plots/roc_auc_interaction_strength.png)

We can observe that for gLV, the AUROC score shows a strong positive correlation with the maximal interaction strength.
No apparent trend can be observed for other methods.

The last experiment I conducted is to explore the effect of the time step ($dt$) on the AUROC score.
Possible time step values are 0.01, 0.1, 0.5 and 1.

![AUROC score on different time step](plots/roc_auc_time_step.png)

We can see that the shorter the time step is, the better the AUROC score is for gLV.
The other methods didn't work.

So far we can see that the trend of AUROC score for gLV conforms to our intuition.
For the next step, I would like to explore the reason that cLV not working on the compositional data, as I talked about it before, the implementation for cLV is not quite well and I would like to see if I can improve it, because theoretically it should be able to work on compositional data.


## April 13th 2023 report

Until now from our last meeting, I have tried center-log ratio (clr) transformation, additive-log ratio (alr) transformation, to preprocess the relative abundance data, and none of them brought any significant improvement to the performance of the methods.
Isometric-log ratio (ilr) transformation is not used because it totally projects the datapoints into a totally different space and we cannot interpret the result out of it.
The result is shown below

![AUROC score with no transformation](plots/rel_org.png)

![AUROC score with clr transformation](plots/rel_clr.png)

![AUROC score with alr transformation](plots/rel_alr.png)

I looked into the scores of these methods, and the highest score was all below 0.57.
I also developed a very prototyped version of a neural network with self-attention to predict the interaction, the reslt was only slightly better than the current methods with 0.58 for 100 timepoints and 0.60 for 500 timepoints.

# References