{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from neuralnet import DNN_pred, Abs_LV, Rel_LV, Compo_LV\n",
    "from utils.evaluations import calc_nondiag_score\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from utils.simulation import simulate_glv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "abundance = np.load(r'D:\\microbial_network\\microbial_network_explore\\data\\simulated\\n20_k5_random_random_1.0_1000\\y.npy')\n",
    "abundance = abundance / abundance.sum(axis=1, keepdims=True)\n",
    "adj = np.load(r'D:\\microbial_network\\microbial_network_explore\\data\\simulated\\n20_k5_random_random_1.0_1000\\adj.npy')\n",
    "adj_norm = adj / adj.sum(axis=1, keepdims=True)\n",
    "M = np.load(r'D:\\microbial_network\\microbial_network_explore\\data\\simulated\\n20_k5_random_random_1.0_1000\\M.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'mgx' initialized\n",
      "Interaction 'mgx->mgx' added\n",
      "set m:(mgx)->(mgx):   0:10    0:10\n",
      "Added x0 vector to node mgx\n",
      "Added growth rates to node mgx\n",
      "Initialized\n",
      "Interaction 'mgx_mgx' added\n"
     ]
    }
   ],
   "source": [
    "z, x, y, adj, M = simulate_glv(\n",
    "        num_taxa=10,\n",
    "        # avg_degree=np.random.randint(5, 20),\n",
    "        avg_degree=2,\n",
    "        time_points=10000,\n",
    "        time_step=0.01,\n",
    "        downsample=1,\n",
    "        noise_var=0,\n",
    "        max_interaction_strength=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN_pred(20, 512).to('cuda')\n",
    "\n",
    "loss_fn = torch.nn.MSELoss().to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10000, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3444, Loss: 1.0419588534205104e-06\u001b[1K\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_num):\n\u001b[0;32m      3\u001b[0m     input_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(abundance[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     target_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(abundance[\u001b[39m1\u001b[39;49m:, :])\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[39m# print(input_data.shape, target_data.shape)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     output \u001b[39m=\u001b[39m model(input_data)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_num = 300000\n",
    "for epoch in range(epoch_num):\n",
    "    input_data = torch.from_numpy(abundance[:-1, :]).float().to('cuda')\n",
    "    target_data = torch.from_numpy(abundance[1:, :]).float().to('cuda')\n",
    "    # print(input_data.shape, target_data.shape)\n",
    "    rel = model(input_data)\n",
    "    loss = loss_fn(rel, target_data)\n",
    "\n",
    "    # identity = torch.eye(output.shape[1]).to('cuda')\n",
    "    # id_out = model(identity)\n",
    "    # lamda = 0.001\n",
    "    # loss = loss + lamda * torch.abs(id_out).sum()\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\", end='\\x1b[1K\\r')\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29848434836800364, 0.5378928571428572]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mat = torch.eye(abundance.shape[1]).to(device)\n",
    "rel = np.abs(model(input_mat).detach().cpu().numpy())\n",
    "calc_nondiag_score(rel, adj, metrics=[average_precision_score, roc_auc_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Abs_LV(10).to('cuda')\n",
    "loss_fn = torch.nn.MSELoss().to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299999, Loss: 1.720613651556846e-09\u001b[1KK\r"
     ]
    }
   ],
   "source": [
    "epoch_num = 300000\n",
    "for epoch in range(epoch_num):\n",
    "    input_data = torch.from_numpy(z[:-1, :]).float().to('cuda')\n",
    "    target_data = torch.from_numpy(z[1:, :]).float().to('cuda')\n",
    "    # print(input_data.shape, target_data.shape)\n",
    "    rel = model(input_data)\n",
    "    loss = loss_fn(rel, target_data)\n",
    "\n",
    "    sparse_loss = torch.norm(model.interaction, p=1) + torch.norm(model.g, p=1)\n",
    "    loss += sparse_loss * 1e-10\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\", end='\\x1b[1K\\r')\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.interaction.data.cpu().numpy()\n",
    "g = model.g.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8813455988455987, 0.92]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38180784990377203, 0.6128571428571429]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'mgx' initialized\n",
      "Interaction 'mgx->mgx' added\n",
      "set m:(mgx)->(mgx):   0:5    0:5\n",
      "Added x0 vector to node mgx\n",
      "Added growth rates to node mgx\n",
      "Initialized\n",
      "Interaction 'mgx_mgx' added\n",
      "Epoch: 189949, Loss: 8.078518476395402e-07\u001b[2KK\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m# print(input_data.shape, target_data.shape)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m output \u001b[39m=\u001b[39m model(input_data)\n\u001b[1;32m---> 27\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, target_data)\n\u001b[0;32m     29\u001b[0m sparse_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(model\u001b[39m.\u001b[39minteraction, p\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mnorm(model\u001b[39m.\u001b[39mg, p\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[39m# sparse_loss = torch.norm(model.interaction, p=1)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\nn\\functional.py:3295\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3292\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m   3294\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[1;32m-> 3295\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "prauc = []\n",
    "rocauc = []\n",
    "for i in range(1):\n",
    "    z, x, y, adj, M = simulate_glv(\n",
    "            num_taxa=5,\n",
    "            # avg_degree=np.random.randint(5, 20),\n",
    "            avg_degree=2,\n",
    "            time_points=1000,\n",
    "            time_step=0.01,\n",
    "            downsample=1,\n",
    "            noise_var=0,\n",
    "            max_interaction_strength=1,\n",
    "            seed=datetime.now().microsecond,\n",
    "\n",
    "        )\n",
    "    model = Abs_LV(5).to('cuda')\n",
    "    loss_fn = torch.nn.MSELoss().to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.9)\n",
    "    epoch_num = 300000\n",
    "    for epoch in range(epoch_num):\n",
    "        input_data = torch.from_numpy(z[:-1, :]).float().to('cuda')\n",
    "        target_data = torch.from_numpy(z[1:, :]).float().to('cuda')\n",
    "        # print(input_data.shape, target_data.shape)\n",
    "        output = model(input_data)\n",
    "        loss = loss_fn(output, target_data)\n",
    "\n",
    "        sparse_loss = torch.norm(model.interaction, p=1) + torch.norm(model.g, p=1)\n",
    "        # sparse_loss = torch.norm(model.interaction, p=1)\n",
    "        loss += sparse_loss * 1e-10\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\", end='\\x1b[2K\\r')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    a = model.interaction.data.cpu().numpy()\n",
    "    # g = model.g.data.cpu().numpy()\n",
    "    pr, roc = calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])\n",
    "    prauc.append(pr)\n",
    "    rocauc.append(roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(prauc))\n",
    "print(np.mean(rocauc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../data/results/abslv_auprc.txt', prauc)\n",
    "np.savetxt('../data/results/abslv_roc.txt', rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "prauc = []\n",
    "rocauc = []\n",
    "for i in range(1):\n",
    "    z, x, y, adj, M = simulate_glv(\n",
    "                num_taxa=5,\n",
    "                # avg_degree=np.random.randint(5, 20),\n",
    "                avg_degree=2,\n",
    "                time_points=1000,\n",
    "                time_step=0.01,\n",
    "                downsample=1,\n",
    "                noise_var=0,\n",
    "                max_interaction_strength=1,\n",
    "            )\n",
    "    model = Rel_LV(5).to('cuda')\n",
    "    # loss_fn = torch.nn.MSELoss().to('cuda')\n",
    "    loss_fn = torch.nn.L1Loss().to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.9)\n",
    "    epoch_num = 300000\n",
    "    for epoch in range(epoch_num):\n",
    "        input_data = torch.from_numpy(x[:-1, :]).float().to('cuda') * 10000\n",
    "        target_data = torch.from_numpy(x[1:, :]).float().to('cuda')\n",
    "        # print(input_data.shape, target_data.shape)\n",
    "        rel = model(input_data, steps=10)\n",
    "        loss = loss_fn(rel, target_data)\n",
    "\n",
    "        # sparse_loss = torch.norm(model.interaction, p=1) + torch.norm(model.g, p=1)\n",
    "        # loss += sparse_loss * 1e-10\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\", end='\\x1b[2K\\r')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    a = model.interaction.data.cpu().numpy()\n",
    "    g = model.g.data.cpu().numpy()\n",
    "    pr, roc = calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])\n",
    "    prauc.append(pr)\n",
    "    rocauc.append(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5071734852694605\n",
      "0.32000000000000006\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(prauc))\n",
    "print(np.mean(rocauc))\n",
    "# np.savetxt('../data/results/rellv_prauc.txt', prauc)\n",
    "# np.savetxt('../data/results/rellv_roc.txt', rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'mgx' initialized\n",
      "Interaction 'mgx->mgx' added\n",
      "set m:(mgx)->(mgx):   0:5    0:5\n",
      "Added x0 vector to node mgx\n",
      "Added growth rates to node mgx\n",
      "Initialized\n",
      "Interaction 'mgx_mgx' added\n",
      "Epoch: 0, Loss: 0.0505216047167778, AUPRC: 0.5854628214922333, AUROC: 0.53\n",
      "Epoch: 500, Loss: 0.008357464335858822, AUPRC: 0.4830460767302872, AUROC: 0.41\n",
      "Epoch: 1000, Loss: 0.0018467504996806383, AUPRC: 0.47842000835421883, AUROC: 0.39\n",
      "Epoch: 1500, Loss: 0.0008301680791191757, AUPRC: 0.4846333783175888, AUROC: 0.39999999999999997\n",
      "Epoch: 2000, Loss: 0.0006143679493106902, AUPRC: 0.49109391124871, AUROC: 0.42000000000000004\n",
      "Epoch: 2500, Loss: 0.0005543466540984809, AUPRC: 0.4937254901960784, AUROC: 0.43\n",
      "Epoch: 3000, Loss: 0.0005325202946551144, AUPRC: 0.4937254901960784, AUROC: 0.43\n",
      "Epoch: 3500, Loss: 0.0005213477415964007, AUPRC: 0.4937254901960784, AUROC: 0.43\n",
      "Epoch: 4000, Loss: 0.0005130813806317747, AUPRC: 0.4937254901960784, AUROC: 0.43\n",
      "Epoch: 4500, Loss: 0.0005054890061728656, AUPRC: 0.4937254901960784, AUROC: 0.43\n",
      "Epoch: 5000, Loss: 0.0004979292280040681, AUPRC: 0.4937254901960784, AUROC: 0.43\n",
      "Epoch: 5500, Loss: 0.000490209145937115, AUPRC: 0.48918003565062385, AUROC: 0.42\n",
      "Epoch: 6000, Loss: 0.00048226345097646117, AUPRC: 0.48918003565062385, AUROC: 0.42\n",
      "Epoch: 6500, Loss: 0.00047405602526851, AUPRC: 0.48918003565062385, AUROC: 0.42\n",
      "Epoch: 7000, Loss: 0.00046555959852412343, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 7500, Loss: 0.0004567436990328133, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 8000, Loss: 0.00044757791329175234, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 8500, Loss: 0.0004380278696771711, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 9000, Loss: 0.00042805285193026066, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 9500, Loss: 0.00041760719614103436, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 10000, Loss: 0.0004066418041475117, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 10500, Loss: 0.0003950941318180412, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 11000, Loss: 0.0003828976477961987, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 11500, Loss: 0.00036997278220951557, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 12000, Loss: 0.00035622951691038907, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 12500, Loss: 0.00034156194305978715, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 13000, Loss: 0.00032585187000222504, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 13500, Loss: 0.0003089632373303175, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 14000, Loss: 0.0002907499438151717, AUPRC: 0.5058467023172906, AUROC: 0.42999999999999994\n",
      "Epoch: 14500, Loss: 0.0002710613189265132, AUPRC: 0.500291146761735, AUROC: 0.41999999999999993\n",
      "Epoch: 15000, Loss: 0.0002497689565643668, AUPRC: 0.500291146761735, AUROC: 0.41999999999999993\n",
      "Epoch: 15500, Loss: 0.0002268203825224191, AUPRC: 0.500291146761735, AUROC: 0.41999999999999993\n",
      "Epoch: 16000, Loss: 0.00020234765543136746, AUPRC: 0.5048366013071895, AUROC: 0.42999999999999994\n",
      "Epoch: 16500, Loss: 0.00017688446678221226, AUPRC: 0.507468180254558, AUROC: 0.43999999999999995\n",
      "Epoch: 17000, Loss: 0.00015166688535828143, AUPRC: 0.507468180254558, AUROC: 0.43999999999999995\n",
      "Epoch: 17500, Loss: 0.00012882631563115865, AUPRC: 0.5108015135878913, AUROC: 0.44999999999999996\n",
      "Epoch: 18000, Loss: 0.00011083843128290027, AUPRC: 0.5069553597417374, AUROC: 0.44\n",
      "Epoch: 18500, Loss: 9.895984112517908e-05, AUPRC: 0.5069553597417374, AUROC: 0.44\n",
      "Epoch: 19000, Loss: 9.222743392456323e-05, AUPRC: 0.5069553597417374, AUROC: 0.44\n",
      "Epoch: 19500, Loss: 8.856346539687365e-05, AUPRC: 0.5098965362123257, AUROC: 0.44999999999999996\n",
      "Epoch: 20000, Loss: 8.63105378812179e-05, AUPRC: 0.5098965362123257, AUROC: 0.44999999999999996\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "prauc = []\n",
    "rocauc = []\n",
    "device = 'cpu'\n",
    "for i in range(1):\n",
    "    z, x, y, adj, M = simulate_glv(\n",
    "                num_taxa=5,\n",
    "                # avg_degree=np.random.randint(5, 20),\n",
    "                avg_degree=2,\n",
    "                time_points=1000,\n",
    "                time_step=0.01,\n",
    "                downsample=1,\n",
    "                noise_var=0,\n",
    "                max_interaction_strength=1,\n",
    "                seed=datetime.now().microsecond,\n",
    "            )\n",
    "    model = Compo_LV(5).to(device)\n",
    "    loss_fn = torch.nn.MSELoss().to(device)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    # optimizer = torch.optim.NAdam(model.parameters(), lr=1e-2)\n",
    "    # optimizer = torch.optim.RAdam(model.parameters(), lr=1e-2)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=5e-2)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.9)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.9)\n",
    "    epoch_num = 1 + 20000\n",
    "    input_data = torch.from_numpy(x[0, :]).float().to(device) * 10\n",
    "    target_data = torch.from_numpy(x[1:, :]).float().to(device)\n",
    "    for epoch in range(epoch_num):\n",
    "        # print(input_data.shape, target_data.shape)\n",
    "        rel = model(input_data, steps=target_data.shape[0])\n",
    "        # output = model(input_data, steps=9)\n",
    "        loss = loss_fn(rel, target_data)\n",
    "        # loss = loss_fn(output[-1,:], target_data[-1,:])\n",
    "\n",
    "        # sparse_loss = torch.norm(model.interaction, p=1)\n",
    "        # loss += sparse_loss * 1e-6\n",
    "        \n",
    "        sys.stdout.write('\\x1b[2K')\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\", end='\\r')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        if epoch % 500 == 0:\n",
    "            a = model.interaction.data.numpy()\n",
    "            g = model.g.data.numpy()\n",
    "            pr, roc = calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}, AUPRC: {pr}, AUROC: {roc}\")\n",
    "    if device == 'cuda':\n",
    "        a = model.interaction.data.cpu().numpy()\n",
    "        g = model.g.data.cpu().numpy()\n",
    "    else:\n",
    "        a = model.interaction.data.numpy()\n",
    "        g = model.g.data.numpy()\n",
    "    pr, roc = calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])\n",
    "    prauc.append(pr)\n",
    "    rocauc.append(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61000, Loss: 3.801764250965789e-06, AUPRC: 0.49977832624891444, AUROC: 0.42\n",
      "Epoch: 61500, Loss: 3.6986893974244595e-06, AUPRC: 0.5036244800950683, AUROC: 0.42999999999999994\n",
      "Epoch: 62000, Loss: 3.5984746773465304e-06, AUPRC: 0.5036244800950683, AUROC: 0.42999999999999994\n",
      "Epoch: 62500, Loss: 3.5009707062272355e-06, AUPRC: 0.5036244800950683, AUROC: 0.42999999999999994\n",
      "Epoch: 63000, Loss: 3.406072892175871e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 63500, Loss: 3.313736215204699e-06, AUPRC: 0.4958467023172905, AUROC: 0.4099999999999999\n",
      "Epoch: 64000, Loss: 3.223942940167035e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 64500, Loss: 3.136662371616694e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 65000, Loss: 3.0517217055603396e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 65500, Loss: 2.969084334836225e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 66000, Loss: 2.88864748654305e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 66500, Loss: 2.8103663680667523e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 67000, Loss: 2.7342280191078316e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 67500, Loss: 2.660078052940662e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 68000, Loss: 2.5880376597342547e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 68500, Loss: 2.5179488147841766e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 69000, Loss: 2.449740804877365e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 69500, Loss: 2.383347009526915e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 70000, Loss: 2.318718088645255e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 70500, Loss: 2.2558508590009296e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 71000, Loss: 2.194613443862181e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 71500, Loss: 2.135109525625012e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 72000, Loss: 2.0772445168404374e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 72500, Loss: 2.020982037720387e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 73000, Loss: 1.9661611077026464e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 73500, Loss: 1.9128769963572267e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 74000, Loss: 1.8610065808388754e-06, AUPRC: 0.4991800356506238, AUROC: 0.42\n",
      "Epoch: 74500, Loss: 1.810543835745193e-06, AUPRC: 0.5021212121212121, AUROC: 0.42999999999999994\n",
      "Epoch: 75000, Loss: 1.7614316902836435e-06, AUPRC: 0.5021212121212121, AUROC: 0.42999999999999994\n",
      "Epoch: 75500, Loss: 1.7136587757704547e-06, AUPRC: 0.5021212121212121, AUROC: 0.42999999999999994\n",
      "Epoch: 76000, Loss: 1.66723589245521e-06, AUPRC: 0.5021212121212121, AUROC: 0.42999999999999994\n",
      "Epoch: 76500, Loss: 1.62201581588306e-06, AUPRC: 0.5021212121212121, AUROC: 0.42999999999999994\n",
      "Epoch: 77000, Loss: 1.5780424291733652e-06, AUPRC: 0.5021212121212121, AUROC: 0.42999999999999994\n",
      "Epoch: 77500, Loss: 1.5352385389633127e-06, AUPRC: 0.5054545454545455, AUROC: 0.44\n",
      "Epoch: 78000, Loss: 1.493590502832376e-06, AUPRC: 0.5054545454545455, AUROC: 0.44\n",
      "Epoch: 78500, Loss: 1.4531822216667933e-06, AUPRC: 0.5054545454545455, AUROC: 0.44\n",
      "Epoch: 79000, Loss: 1.4138079222902888e-06, AUPRC: 0.5054545454545455, AUROC: 0.44\n",
      "Epoch: 79500, Loss: 1.3754990959569113e-06, AUPRC: 0.5054545454545455, AUROC: 0.44\n",
      "Epoch: 80000, Loss: 1.338231982117577e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 80500, Loss: 1.3020099913774175e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 81000, Loss: 1.2668098179346998e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 81500, Loss: 1.2325385796430055e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 82000, Loss: 1.1991791097898385e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 82500, Loss: 1.1667523267533397e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 83000, Loss: 1.1351870625730953e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 83500, Loss: 1.1044553502870258e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 84000, Loss: 1.074520127986034e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 84500, Loss: 1.0454416496941121e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 85000, Loss: 1.0170982704948983e-06, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 85500, Loss: 9.895778703139513e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 86000, Loss: 9.6280814432248e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 86500, Loss: 9.367581697006244e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 87000, Loss: 9.113922487813397e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 87500, Loss: 8.867372116583283e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 88000, Loss: 8.627372380942688e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 88500, Loss: 8.393964776587381e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 89000, Loss: 8.166207408066839e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 89500, Loss: 7.945401421238785e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 90000, Loss: 7.730238280601043e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 90500, Loss: 7.520883400502498e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 91000, Loss: 7.317242420867842e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 91500, Loss: 7.119240308384178e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 92000, Loss: 6.927050435479032e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 92500, Loss: 6.739750233464292e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 93000, Loss: 6.557921210514905e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 93500, Loss: 6.380599302246992e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 94000, Loss: 6.208252898431965e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 94500, Loss: 6.040543780727603e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 95000, Loss: 5.877211606275523e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 95500, Loss: 5.718512738894788e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 96000, Loss: 5.563711624745338e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 96500, Loss: 5.413652388597257e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 97000, Loss: 5.267276037557167e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 97500, Loss: 5.124747417539766e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 98000, Loss: 4.986289354746987e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 98500, Loss: 4.851373205383425e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 99000, Loss: 4.720351967080205e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 99500, Loss: 4.5930119085824117e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 100000, Loss: 4.4690008849102014e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 100500, Loss: 4.3483518652465136e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 101000, Loss: 4.230965657825436e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 101500, Loss: 4.116509444429539e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 102000, Loss: 4.005434561804577e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 102500, Loss: 3.8972734728304204e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 103000, Loss: 3.791870710756484e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 103500, Loss: 3.689523850880505e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 104000, Loss: 3.589883306176489e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 104500, Loss: 3.4926634384646604e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 105000, Loss: 3.398233161533426e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 105500, Loss: 3.306342080122704e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 106000, Loss: 3.2168026109502534e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 106500, Loss: 3.129726167117042e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 107000, Loss: 3.0450993904196366e-07, AUPRC: 0.5083957219251337, AUROC: 0.44999999999999996\n",
      "Epoch: 107500, Loss: 2.9626770015056536e-07, AUPRC: 0.5122418757712875, AUROC: 0.45999999999999996\n",
      "Epoch: 108000, Loss: 2.882275964566361e-07, AUPRC: 0.5122418757712875, AUROC: 0.45999999999999996\n",
      "Epoch: 108500, Loss: 2.80462671753412e-07, AUPRC: 0.5122418757712875, AUROC: 0.45999999999999996\n",
      "Epoch: 109000, Loss: 2.7289689796816674e-07, AUPRC: 0.5122418757712875, AUROC: 0.45999999999999996\n",
      "Epoch: 109500, Loss: 2.655150126429362e-07, AUPRC: 0.5122418757712875, AUROC: 0.45999999999999996\n",
      "Epoch: 110000, Loss: 2.5833719519141596e-07, AUPRC: 0.5122418757712875, AUROC: 0.45999999999999996\n",
      "Epoch: 110500, Loss: 2.5136097292488557e-07, AUPRC: 0.5122418757712875, AUROC: 0.45999999999999996\n",
      "\u001b[2KEpoch: 110609, Loss: 2.498677247331216e-077\r"
     ]
    }
   ],
   "source": [
    "# Continue\n",
    "start_point = epoch + 1\n",
    "input_data = torch.from_numpy(x[0, :]).float().to(device)\n",
    "target_data = torch.from_numpy(x[1:, :]).float().to(device)\n",
    "for epoch in range(start_point, start_point + 50000):\n",
    "        # print(input_data.shape, target_data.shape)\n",
    "        rel = model(input_data, steps=target_data.shape[0])\n",
    "        # output = model(input_data, steps=9)\n",
    "        # loss = loss_fn(output, target_data)\n",
    "        loss = loss_fn(rel[-1,:], target_data[-1,:])\n",
    "\n",
    "        # sparse_loss = torch.norm(model.interaction, p=1)\n",
    "        # loss += sparse_loss * 1e-6\n",
    "        \n",
    "        sys.stdout.write('\\x1b[2K')\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\", end='\\r')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        if epoch % 500 == 0:\n",
    "            a = model.interaction.data.numpy()\n",
    "            g = model.g.data.numpy()\n",
    "            pr, roc = calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}, AUPRC: {pr}, AUROC: {roc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'mgx' initialized\n",
      "Interaction 'mgx->mgx' added\n",
      "set m:(mgx)->(mgx):   0:5    0:5\n",
      "Added x0 vector to node mgx\n",
      "Added growth rates to node mgx\n",
      "Initialized\n",
      "Interaction 'mgx_mgx' added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\misdeed-1.0.2-py3.11.egg\\misdeed\\OmicsGenerator.py:802: RuntimeWarning: overflow encountered in multiply\n",
      "c:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\scipy\\integrate\\_ivp\\common.py:112: RuntimeWarning: invalid value encountered in multiply\n",
      "  y1 = y0 + h0 * direction * f0\n",
      "c:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\scipy\\integrate\\_ivp\\common.py:114: RuntimeWarning: invalid value encountered in subtract\n",
      "  d2 = norm((f1 - f0) / scale) / h0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: nan, AUPRC: 0.5196428571428572, AUROC: 0.56                                         \n",
      "Epoch: 3, Loss: nan                                                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[39m# sparse_loss = torch.norm(model.interaction, p=1)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[39m# loss += sparse_loss * 1e-5  \u001b[39;00m\n\u001b[0;32m     46\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 47\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     48\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     50\u001b[0m \u001b[39m# sparse_loss = torch.norm(model.interaction, p=1)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m# loss += sparse_loss * 1e-6\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[0;32m     53\u001b[0m \u001b[39m# sys.stdout.write('\\x1b[2K')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = open('D:\\microbial_network\\microbial_network_explore\\data\\output.txt', 'w')\n",
    "prauc = []\n",
    "rocauc = []\n",
    "device = 'cpu'\n",
    "for i in range(1):\n",
    "    z, x, y, adj, M = simulate_glv(\n",
    "                num_taxa=5,\n",
    "                avg_degree=3,\n",
    "                time_points=1000,\n",
    "                time_step=0.01,\n",
    "                downsample=1,\n",
    "                noise_var=0,\n",
    "                max_interaction_strength=1,\n",
    "                seed=datetime.now().microsecond,\n",
    "            )\n",
    "    model = Compo_LV(5).to(device)\n",
    "    loss_fn = torch.nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-6)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=1e-6)\n",
    "    # epoch_num = 1 + 20000\n",
    "    epoch_num = 1 + 300000\n",
    "    # input_data = torch.from_numpy(x[0, :]).float().to(device)\n",
    "    # target_data = torch.from_numpy(x[1:, :]).float().to(device)\n",
    "    x = torch.from_numpy(x).float().to(device)\n",
    "    for epoch in range(epoch_num):\n",
    "        input_data = x[0, :] * 10\n",
    "        target_data = x[1, :]\n",
    "        abs, rel = model(input_data)\n",
    "        # print(abs.sum())\n",
    "        loss = loss_fn(rel, target_data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        for i in range(1, x.shape[0]-1):\n",
    "            abs = abs.detach()\n",
    "            # while abs.sum() > 10:\n",
    "            #     abs = abs / 10\n",
    "            # print(abs.sum())\n",
    "            input_data = abs.sum() * x[i, :]\n",
    "            target_data = x[i+1, :]\n",
    "            abs, rel = model(input_data)\n",
    "            loss = loss_fn(rel, target_data)\n",
    "            # sparse_loss = torch.norm(model.interaction, p=1)\n",
    "            # loss += sparse_loss * 1e-5  \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # sparse_loss = torch.norm(model.interaction, p=1)\n",
    "        # loss += sparse_loss * 1e-6\n",
    "        \n",
    "        # sys.stdout.write('\\x1b[2K')\n",
    "        print(\" \" * 100, end='\\r')\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\", end='\\r')\n",
    "        \n",
    "        # scheduler.step()\n",
    "        if epoch % 100 == 0:\n",
    "            a = model.interaction.data.numpy()\n",
    "            g = model.g.data.numpy()\n",
    "            pr, roc = calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])\n",
    "            # print(f\"Epoch: {epoch}, Loss: {loss.item()}, AUPRC: {pr}, AUROC: {roc}\", file=f, flush=True)\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}, AUPRC: {pr}, AUROC: {roc}\")\n",
    "    if device == 'cuda':\n",
    "        a = model.interaction.data.cpu().numpy()\n",
    "        g = model.g.data.cpu().numpy()\n",
    "    else:\n",
    "        a = model.interaction.data.numpy()\n",
    "        g = model.g.data.numpy()\n",
    "    pr, roc = calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])\n",
    "    prauc.append(pr)\n",
    "    rocauc.append(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'mgx' initialized\n",
      "Interaction 'mgx->mgx' added\n",
      "set m:(mgx)->(mgx):   0:5    0:5\n",
      "Added x0 vector to node mgx\n",
      "Added growth rates to node mgx\n",
      "Initialized\n",
      "Interaction 'mgx_mgx' added\n",
      "Epoch: 0, Loss: 768.8861694335938, AUPRC: 0.5603407213701331, AUROC: 0.56\n",
      "Epoch: 5000, Loss: 5.991274883854203e-05, AUPRC: 0.7152292070674423, AUROC: 0.65\n",
      "Epoch: 10000, Loss: 0.005935227498412132, AUPRC: 0.6613690476190476, AUROC: 0.6000000000000001\n",
      "Epoch: 15000, Loss: 1.4007032405061182e-05, AUPRC: 0.708015873015873, AUROC: 0.65\n",
      "Epoch: 20000, Loss: 0.01743469014763832, AUPRC: 0.7262148962148962, AUROC: 0.72\n",
      "Epoch: 25000, Loss: 0.0008301378111355007, AUPRC: 0.699594017094017, AUROC: 0.66\n",
      "Epoch: 30000, Loss: 0.0031353295780718327, AUPRC: 0.6186999550157445, AUROC: 0.5599999999999999\n",
      "Epoch: 35000, Loss: 4.223721134621883e-06, AUPRC: 0.5667077418625406, AUROC: 0.51\n",
      "Epoch: 40000, Loss: 2.932983079517726e-06, AUPRC: 0.568489010989011, AUROC: 0.53\n",
      "Epoch: 45000, Loss: 0.007312569301575422, AUPRC: 0.5964301874595992, AUROC: 0.53\n",
      "Epoch: 50000, Loss: 0.000253223319305107, AUPRC: 0.5673317910160015, AUROC: 0.46\n",
      "Epoch: 55000, Loss: 7.3131163844664115e-06, AUPRC: 0.5856395891690009, AUROC: 0.52\n",
      "Epoch: 60000, Loss: 0.001399477943778038, AUPRC: 0.6097610671062683, AUROC: 0.55\n",
      "Epoch: 65000, Loss: 0.04470719024538994, AUPRC: 0.5967094017094017, AUROC: 0.5\n",
      "Epoch: 70000, Loss: 0.007673406973481178, AUPRC: 0.6292844655344655, AUROC: 0.5700000000000001\n",
      "Epoch: 75000, Loss: 0.0015560865867882967, AUPRC: 0.6184865573023467, AUROC: 0.54\n",
      "Epoch: 80000, Loss: 4.46926242148038e-06, AUPRC: 0.624250700280112, AUROC: 0.55\n",
      "Epoch: 85000, Loss: 8.261622133431956e-05, AUPRC: 0.6298062558356675, AUROC: 0.5700000000000001\n",
      "Epoch: 90000, Loss: 1.2302158211241476e-05, AUPRC: 0.6298062558356675, AUROC: 0.5700000000000001\n",
      "Epoch: 95000, Loss: 0.0015606933739036322, AUPRC: 0.6309173669467787, AUROC: 0.56\n",
      "Epoch: 100000, Loss: 3.808618930634111e-05, AUPRC: 0.6298062558356675, AUROC: 0.5700000000000001\n",
      "Epoch: 105000, Loss: 0.00027534752734936774, AUPRC: 0.6298062558356675, AUROC: 0.5700000000000001\n",
      "Epoch: 110000, Loss: 1.8704304238781333e-06, AUPRC: 0.6298062558356675, AUROC: 0.5700000000000001\n",
      "Epoch: 115000, Loss: 1.260139242731384e-06, AUPRC: 0.6298062558356675, AUROC: 0.5700000000000001\n",
      "Epoch: 120000, Loss: 5.85690077059553e-06, AUPRC: 0.6280968541262658, AUROC: 0.5599999999999999\n",
      "Epoch: 122892, Loss: 3.3480904676252976e-05\u001b[2K\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     45\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 46\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     47\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m5000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\dorasir\\anaconda3\\envs\\net-simu\\lib\\site-packages\\torch\\optim\\adam.py:445\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    442\u001b[0m     device_grads \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_add(device_grads, device_params, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m    444\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m torch\u001b[39m.\u001b[39;49m_foreach_mul_(device_exp_avgs, beta1)\n\u001b[0;32m    446\u001b[0m torch\u001b[39m.\u001b[39m_foreach_add_(device_exp_avgs, device_grads, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m    448\u001b[0m torch\u001b[39m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "def clr_transform(data):\n",
    "    # Calculate the geometric mean of each row\n",
    "    gm = np.exp(np.mean(np.log(data), axis=1, keepdims=True))\n",
    "\n",
    "    # Calculate the CLR-transformed values\n",
    "    # clr = np.log(data / gm)\n",
    "    clr = data / gm\n",
    "\n",
    "    return clr\n",
    "prauc = []\n",
    "rocauc = []\n",
    "for i in range(1):\n",
    "    z, x, y, adj, M = simulate_glv(\n",
    "            num_taxa=5,\n",
    "            # avg_degree=np.random.randint(5, 20),\n",
    "            avg_degree=2,\n",
    "            time_points=1000,\n",
    "            time_step=0.01,\n",
    "            downsample=1,\n",
    "            noise_var=0,\n",
    "            max_interaction_strength=1,\n",
    "            seed=datetime.now().microsecond,\n",
    "\n",
    "        )\n",
    "    model = Abs_LV(5).to('cuda')\n",
    "    loss_fn = torch.nn.MSELoss().to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.9)\n",
    "    epoch_num = 300000\n",
    "    z = clr_transform(x)\n",
    "    for epoch in range(epoch_num):\n",
    "        input_data = torch.from_numpy(z[:-1, :]).float().to('cuda')\n",
    "        target_data = torch.from_numpy(z[1:, :]).float().to('cuda')\n",
    "        # print(input_data.shape, target_data.shape)\n",
    "        output = model(input_data)\n",
    "        loss = loss_fn(output, target_data)\n",
    "\n",
    "        sparse_loss = torch.norm(model.interaction, p=1) + torch.norm(model.g, p=1)\n",
    "        # sparse_loss = torch.norm(model.interaction, p=1)\n",
    "        loss += sparse_loss * 1e-10\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\", end='\\x1b[2K\\r')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if epoch % 5000 == 0:\n",
    "            a = model.interaction.data.cpu().numpy()\n",
    "            g = model.g.data.cpu().numpy()\n",
    "            pr, roc = calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])\n",
    "            # print(f\"Epoch: {epoch}, Loss: {loss.item()}, AUPRC: {pr}, AUROC: {roc}\", file=f, flush=True)\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}, AUPRC: {pr}, AUROC: {roc}\")\n",
    "    a = model.interaction.data.cpu().numpy()\n",
    "    # g = model.g.data.cpu().numpy()\n",
    "    pr, roc = calc_nondiag_score(np.abs(a), adj, metrics=[average_precision_score, roc_auc_score])\n",
    "    prauc.append(pr)\n",
    "    rocauc.append(roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5883141994171406\n",
      "0.32999999999999996\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(prauc))\n",
    "print(np.mean(rocauc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare ours and their program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "net-simu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
