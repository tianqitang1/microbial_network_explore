{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Jupyter modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the working directory is correct\n",
    "import os\n",
    "os.chdir('d:\\\\microbial_network\\\\microbial_network_explore')\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import pandas as pd\n",
    "from utils.transformation import clr_transform, alr_transform\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from utils import simulation\n",
    "import rpy2.robjects as robjects\n",
    "import seaborn as sns\n",
    "from utils.generalized_lotka_volterra import GeneralizedLotkaVolterra\n",
    "from utils.compositional_lotka_volterra import CompositionalLotkaVolterra\n",
    "from scipy.stats import ttest_rel\n",
    "import utils.evaluations as ev\n",
    "from utils.evaluations import correlation_score, spearman_score, precision_matrix_score, clv_score, glv_score, pcor_score, pspe_score, sparcc_score, speic_score, cclasso_score, baseline_score\n",
    "from typing import List\n",
    "import seaborn as sns\n",
    "from utils.transformation import clr_transform, alr_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for evaluation\n",
    "def evaluation(adj, abundance, evaluation_func, metrics=average_precision_score, verbose=False, f=None):\n",
    "    scores = []\n",
    "    for func in evaluation_func:\n",
    "        try:\n",
    "            scores.append([func._method, *func(abundance, adj, metrics=metrics, verbose=verbose)])\n",
    "        except Exception as e:\n",
    "            scores.append([func._method, *[0] * len(metrics)])\n",
    "            print(f\"Error: {e}\")\n",
    "        if f:\n",
    "            f.write(f\"{func._method}\\n\")\n",
    "            f.flush()\n",
    "    columns = ['Method']\n",
    "    columns.extend([metric.__name__ for metric in metrics] if isinstance(metrics, List) else [metrics.__name__])\n",
    "    scores_df = pd.DataFrame(scores, columns=columns)\n",
    "    return scores_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the parameters for the first simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "n_vertices = 50\n",
    "avg_degree = 2\n",
    "network_type = 'random'\n",
    "interaction_type = 'random'\n",
    "max_interaction_strength = 1\n",
    "time_points = 500\n",
    "time_step = 0.01\n",
    "downsample = 1\n",
    "noise_var = 1e-3\n",
    "\n",
    "# evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, pcor_score, pspe_score, sparcc_score, speic_score, ev.dl_score, baseline_score]\n",
    "evaluation_func = [correlation_score, ev.dl_score, baseline_score]\n",
    "metrics = [average_precision_score, roc_auc_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "result_df = pd.DataFrame(columns=[\"Method\", \"run\", \"average_precision_score\", \"roc_auc_score\", \"abs_rel\"])\n",
    "repeat = 50\n",
    "# Setup the random seed generator\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "f = open(\"data\\\\results\\\\log\", \"w\")\n",
    "\n",
    "for i in range(repeat):\n",
    "    seed = rng.integers(0, 2**32 - 1)\n",
    "    adj, M = simulation.gen_graph(n_vertices, avg_degree, network_type, interaction_type, max_interaction_strength=1, seed=seed)\n",
    "    z, x, abd, _, _ = simulation.simulate_glv(\n",
    "        time_points=time_points,\n",
    "        time_step=time_step,\n",
    "        downsample=1,\n",
    "        noise_var=0,\n",
    "        adj=adj,\n",
    "        M=M,\n",
    "    )\n",
    "    f.write(f'iter:{i}\\n')\n",
    "    score_df = evaluation(adj, z, evaluation_func, metrics=metrics, verbose=False, f=f)\n",
    "    score_df[\"run\"] = i\n",
    "    score_df[\"abs_rel\"] = \"Absolute\"\n",
    "    result_df = result_df.append(score_df, ignore_index=True)\n",
    "\n",
    "    score_df = evaluation(adj, abd, evaluation_func, metrics=metrics, verbose=False, f=f)\n",
    "    score_df[\"run\"] = i\n",
    "    score_df[\"abs_rel\"] = \"Relative\"\n",
    "    result_df = result_df.append(score_df, ignore_index=True)\n",
    "    \n",
    "    # result_df.to_csv(\"data\\\\results\\\\simulation_results_fixed_interaction.csv\", index=False)\n",
    "# result_df = pd.read_csv(\"data\\\\temp_results\\\\simulation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5012054468085105\n",
      "0.042884843356203836\n",
      "0.4927114893617021\n",
      "0.042441699746670045\n"
     ]
    }
   ],
   "source": [
    "# print(result_df.loc[result_df[\"Method\"] == \"gLV\", \"roc_auc_score\"].mean())\n",
    "# print(result_df.loc[result_df[\"Method\"] == \"gLV\", \"average_precision_score\"].mean())\n",
    "print(result_df.loc[result_df[\"Method\"] == \"Attention\", \"roc_auc_score\"].mean())\n",
    "print(result_df.loc[result_df[\"Method\"] == \"Attention\", \"average_precision_score\"].mean())\n",
    "print(result_df.loc[result_df[\"Method\"] == \"Baseline\", \"roc_auc_score\"].mean())\n",
    "print(result_df.loc[result_df[\"Method\"] == \"Baseline\", \"average_precision_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WilcoxonResult(statistic=1910.0, pvalue=0.03446663167697859)\n",
      "WilcoxonResult(statistic=2369.0, pvalue=0.5916968660875195)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "print(wilcoxon(result_df.loc[result_df[\"Method\"] == \"Baseline\", \"roc_auc_score\"], result_df.loc[result_df[\"Method\"] == \"Attention\", \"roc_auc_score\"]))\n",
    "print(wilcoxon(result_df.loc[result_df[\"Method\"] == \"Baseline\", \"average_precision_score\"], result_df.loc[result_df[\"Method\"] == \"Attention\", \"average_precision_score\"]))\n",
    "# print(wilcoxon(result_df.loc[result_df[\"Method\"] == \"Baseline\", \"roc_auc_score\"], result_df.loc[result_df[\"Method\"] == \"gLV\", \"roc_auc_score\"]))\n",
    "# print(wilcoxon(result_df.loc[result_df[\"Method\"] == \"Baseline\", \"average_precision_score\"], result_df.loc[result_df[\"Method\"] == \"gLV\", \"average_precision_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "result_df = pd.DataFrame(columns=[\"Method\", \"run\", \"average_precision_score\", \"roc_auc_score\"])\n",
    "repeat = 50\n",
    "# Setup the random seed generator\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for i in range(repeat):\n",
    "    seed = rng.integers(0, 2**32 - 1)\n",
    "    abd, adj, M = simulation.simulate_noiseless_glv(\n",
    "        num_taxa=n_vertices,\n",
    "        avg_degree=avg_degree,\n",
    "        time_points=time_points,\n",
    "        downsample=1,\n",
    "        seed=seed,\n",
    "    )\n",
    "    score_df = evaluation(adj, abd, evaluation_func, metrics=metrics, verbose=False)\n",
    "    score_df[\"run\"] = i\n",
    "    result_df = result_df.append(score_df, ignore_index=True)\n",
    "result_df.to_csv(\"data\\\\results\\\\noiseless_results.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "n_vertices = 5\n",
    "avg_degree = 2\n",
    "network_type = 'random'\n",
    "interaction_type = 'random'\n",
    "max_interaction_strength = 1\n",
    "time_points = [100, 200, 500, 1000, 2000, 3000, 5000]\n",
    "time_step = 0.01\n",
    "downsample = 1\n",
    "noise_var = 1e-3\n",
    "\n",
    "# evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, pcor_score, pspe_score, sparcc_score, speic_score, baseline_score]\n",
    "evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, baseline_score]\n",
    "metrics = [average_precision_score, roc_auc_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Initialize a DataFrame to store the results from each run.\n",
    "result_df = pd.DataFrame(columns=[\"Method\", \"run\", \"average_precision_score\", \"roc_auc_score\", \"time_points\"])\n",
    "# Set the number of times the simulation will be run.\n",
    "repeat = 50\n",
    "# Setup the random seed generator, initializing it with seed 42.\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for time_point in time_points:\n",
    "    for i in range(repeat):\n",
    "        seed = rng.integers(0, 2**32 - 1)\n",
    "        z, x, abd, adj, M = simulation.simulate_glv(\n",
    "            num_taxa=n_vertices,\n",
    "            avg_degree=avg_degree,\n",
    "            time_points=time_point,\n",
    "            time_step=time_step,\n",
    "            downsample=1,\n",
    "            noise_var=0,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        score_df = evaluation(adj, z, evaluation_func, metrics=metrics, verbose=False)\n",
    "        score_df[\"run\"] = i\n",
    "        score_df[\"time_points\"] = time_point\n",
    "        result_df = result_df.append(score_df, ignore_index=True)\n",
    "    result_df.to_csv(\"data\\\\temp_results\\\\vary_timepoints.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "n_vertices = 5\n",
    "avg_degree = 2\n",
    "network_type = 'random'\n",
    "interaction_type = 'random'\n",
    "max_interaction_strength = 1\n",
    "time_points = 500\n",
    "total_time = 50\n",
    "# time_steps = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "time_steps = [0.001, 0.01, 0.1, 1]\n",
    "downsample = 1\n",
    "noise_var = 1e-3\n",
    "\n",
    "# evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, pcor_score, pspe_score, sparcc_score, speic_score, baseline_score]\n",
    "evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, baseline_score]\n",
    "metrics = [average_precision_score, roc_auc_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "result_df = pd.DataFrame(columns=[\"Method\", \"run\", \"average_precision_score\", \"roc_auc_score\", \"time_interval\"])\n",
    "repeat = 1\n",
    "# Setup the random seed generator\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for i in range(repeat):\n",
    "    for intv in time_steps:\n",
    "        seed = rng.integers(0, 2**32 - 1)\n",
    "        z, x, abd, adj, M = simulation.simulate_glv(\n",
    "            num_taxa=n_vertices,\n",
    "            avg_degree=avg_degree,\n",
    "            time_points=int(total_time/intv),\n",
    "            time_step=intv,\n",
    "            downsample=1,\n",
    "            noise_var=0,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        score_df = evaluation(adj, z, evaluation_func, metrics=metrics, verbose=False)\n",
    "        score_df[\"run\"] = i\n",
    "        score_df[\"time_interval\"] = intv\n",
    "        result_df = result_df.append(score_df, ignore_index=True)\n",
    "    result_df.to_csv(\"data\\\\results\\\\vary_time_interval_230615.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test number of taxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "n_vertices = [5, 10, 20, 50, 100]\n",
    "avg_degree = 2\n",
    "network_type = 'random'\n",
    "interaction_type = 'random'\n",
    "max_interaction_strength = 1\n",
    "time_points = 500\n",
    "time_step = 0.01\n",
    "downsample = 1\n",
    "noise_var = 1e-3\n",
    "\n",
    "# evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, pcor_score, pspe_score, sparcc_score, speic_score, baseline_score]\n",
    "evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, baseline_score]\n",
    "metrics = [average_precision_score, roc_auc_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "result_df = pd.DataFrame(columns=[\"Method\", \"run\", \"average_precision_score\", \"roc_auc_score\", \"num_taxa\"])\n",
    "repeat = 50\n",
    "# Setup the random seed generator\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for i in range(repeat):\n",
    "    for n in n_vertices:\n",
    "        seed = rng.integers(0, 2**32 - 1)\n",
    "        z, x, abd, adj, M = simulation.simulate_glv(\n",
    "            num_taxa=n,\n",
    "            avg_degree=avg_degree,\n",
    "            time_points=time_points,\n",
    "            time_step=intv,\n",
    "            downsample=1,\n",
    "            noise_var=0,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        score_df = evaluation(adj, z, evaluation_func, metrics=metrics, verbose=False)\n",
    "        score_df[\"run\"] = i\n",
    "        score_df[\"num_taxa\"] = n\n",
    "        result_df = result_df.append(score_df, ignore_index=True)\n",
    "    result_df.to_csv(\"data\\\\temp_results\\\\vary_num_taxa.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test number of average degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "n_vertices = 50\n",
    "avg_degree = [2, 5, 10]\n",
    "network_type = 'random'\n",
    "interaction_type = 'random'\n",
    "max_interaction_strength = 1\n",
    "time_points = 500\n",
    "time_step = 0.01\n",
    "downsample = 1\n",
    "noise_var = 1e-3\n",
    "\n",
    "# evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, pcor_score, pspe_score, sparcc_score, speic_score, baseline_score]\n",
    "evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, baseline_score]\n",
    "metrics = [average_precision_score, roc_auc_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "result_df = pd.DataFrame(columns=[\"Method\", \"run\", \"average_precision_score\", \"roc_auc_score\", \"avg_degree\"])\n",
    "repeat = 50\n",
    "# Setup the random seed generator\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for i in range(repeat):\n",
    "    for n in n_vertices:\n",
    "        seed = rng.integers(0, 2**32 - 1)\n",
    "        z, x, abd, adj, M = simulation.simulate_glv(\n",
    "            num_taxa=n,\n",
    "            avg_degree=avg_degree,\n",
    "            time_points=time_points,\n",
    "            time_step=intv,\n",
    "            downsample=1,\n",
    "            noise_var=0,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        score_df = evaluation(adj, z, evaluation_func, metrics=metrics, verbose=False)\n",
    "        score_df[\"run\"] = i\n",
    "        score_df[\"num_taxa\"] = n\n",
    "        result_df = result_df.append(score_df, ignore_index=True)\n",
    "    result_df.to_csv(\"data\\\\temp_results\\\\vary_avg_degree.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test type of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "n_vertices = 50\n",
    "avg_degree = 5\n",
    "# n_vertices = 50\n",
    "# avg_degree = 25\n",
    "network_type = ['random', 'small-world', 'scale-free']\n",
    "interaction_type = 'random'\n",
    "max_interaction_strength = 1\n",
    "time_points = 500\n",
    "time_step = 0.01\n",
    "downsample = 1\n",
    "noise_var = 1e-3\n",
    "\n",
    "# evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, pcor_score, pspe_score, sparcc_score, speic_score, baseline_score]\n",
    "evaluation_func = [correlation_score, spearman_score, clv_score, glv_score, baseline_score]\n",
    "metrics = [average_precision_score, roc_auc_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "result_df = pd.DataFrame(columns=[\"Method\", \"run\", \"average_precision_score\", \"roc_auc_score\", \"network_type\"])\n",
    "repeat = 50\n",
    "# Setup the random seed generator\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for i in range(repeat):\n",
    "    for n in network_type:\n",
    "        seed = rng.integers(0, 2**32 - 1)\n",
    "        z, x, abd, adj, M = simulation.simulate_glv(\n",
    "            num_taxa=n_vertices,\n",
    "            avg_degree=avg_degree,\n",
    "            time_points=time_points,\n",
    "            time_step=time_step,\n",
    "            downsample=1,\n",
    "            noise_var=0,\n",
    "            seed=seed,\n",
    "            network_type=n,\n",
    "        )\n",
    "\n",
    "        score_df = evaluation(adj, z, evaluation_func, metrics=metrics, verbose=False)\n",
    "        score_df[\"run\"] = i\n",
    "        score_df[\"network_type\"] = n\n",
    "        result_df = result_df.append(score_df, ignore_index=True)\n",
    "    result_df.to_csv(\"data\\\\temp_results\\\\vary_network_type.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "net-simu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbe25c87dd4947f7df6284cb2d993a284cf54cb5c7f845d17bcee5047f248a55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
